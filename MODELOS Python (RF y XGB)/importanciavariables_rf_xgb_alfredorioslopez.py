# -*- coding: utf-8 -*-
"""Importanciavariables_RF_XGB_AlfredoRiosLopez.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y4CmNQCaDq_SeTGiD7-vmX1UwyMt0qIb

**Modelo con hiperparámetro random forest**
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# CARGA DE DATOS
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
file_path = "/content/DatosTempHKObs__Finales.xlsx"
data = pd.read_excel(file_path)
data['Date'] = pd.to_datetime(data['Date'])

# Eliminar columnas no deseadas
cols_to_drop = ['Punto de rocio', 'Year', 'Month', 'Day']
data = data.drop(columns=[col for col in cols_to_drop if col in data.columns])

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# DIVISIÓN ENTRE TRAIN Y TEST
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
train_data = data[data['Date'] < '2012-01-01']
test_data = data[data['Date'] >= '2012-01-01']

# Variables predictoras
features = ['Nubes', 'Humedad ', 'Precipitacion', 'Presion ']
X_train = train_data[features]
X_test = test_data[features]

# Variables objetivo
targets = ['MaxTemp', 'MinTemp', 'MeanTemp']

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# OPTIMIZACIÓN DE HYPERPARÁMETROS
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
tscv = TimeSeriesSplit(n_splits=3)
def tune_random_forest(X, y):
    param_grid = {
        'n_estimators': [100, 200],
        'max_depth': [15, 20],
        'min_samples_split': [2, 5],
        'min_samples_leaf': [1, 2],
        'max_features': ['sqrt', 'log2'],
        'bootstrap': [True, False]
    }

    rf = RandomForestRegressor(random_state=42)
    grid_search = GridSearchCV(rf, param_grid, cv=tscv, scoring='r2', n_jobs=2, verbose=1)
    grid_search.fit(X, y)

    print("Mejores parámetros encontrados:", grid_search.best_params_)
    return grid_search.best_estimator_

# Lista para guardar las métricas
metrics_summary = []

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# FUNCIÓN DE RANDOM FOREST + MÉTRICAS Y GRÁFICOS
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
def evaluate_rf_model(y_col):
    y_train = train_data[y_col]
    y_test = test_data[y_col]

    print(f"\n Optimizando hiperparámetros para: {y_col}")
    rf = tune_random_forest(X_train, y_train)

    y_train_pred = rf.predict(X_train)
    y_test_pred = rf.predict(X_test)

    # MÉTRICAS
    def report_metrics(y_true, y_pred, label):
        r2 = r2_score(y_true, y_pred)
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        mask = y_true != 0
        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
        print(f"\n{label} - {y_col}")
        print(f"  R²:   {r2:.3f}")
        print(f"  MAE:  {mae:.3f}")
        print(f"  RMSE: {rmse:.3f}")
        print(f"  MAPE:  {mape:.2f}%")
        return r2, mae, rmse, mape

    r2_test, mae_test, rmse_test, mape_test = report_metrics(y_test, y_test_pred, "Test")
    report_metrics(y_train, y_train_pred, "Train")

    # Guardar métricas
    metrics_summary.append({
        "Variable": y_col,
        "R2_Test": round(r2_test, 3),
        "MAE_Test": round(mae_test, 3),
        "RMSE_Test": round(rmse_test, 3)
    })

    # IMPORTANCIA DE VARIABLES
    importances = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values()
    plt.figure(figsize=(8, 5))
    importances.plot(kind='barh', color='forestgreen')
    plt.title(f"Importancia de Variables - {y_col}")
    plt.xlabel("Importancia")
    plt.tight_layout()
    plt.show()

    # GRÁFICO: REAL VS PREDICHO (TEST)
    plt.figure(figsize=(10, 4))
    plt.plot(test_data['Date'], y_test.values, label="Real", color="steelblue")
    plt.plot(test_data['Date'], y_test_pred, label="Predicción", color="darkred")
    plt.fill_between(test_data['Date'], y_test_pred, y_test.values, color="gray", alpha=0.2, label="Error")
    plt.title(f"Predicción vs Real - {y_col}")
    plt.xlabel("Fecha")
    plt.ylabel("Valor")
    plt.legend()
    plt.tight_layout()
    plt.show()

    # GRÁFICO: ERRORES/RESIDUOS
    residuals = y_test.values - y_test_pred
    plt.figure(figsize=(8, 4))
    plt.plot(test_data['Date'], residuals, color='orange', label='Error')
    plt.axhline(0, color='black', linestyle='--')
    plt.title(f"Errores (Residuos) - {y_col}")
    plt.xlabel("Fecha")
    plt.ylabel("Error")
    plt.legend()
    plt.tight_layout()
    plt.show()

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# EVALUAR CADA VARIABLE OBJETIVO
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
for target in targets:
    evaluate_rf_model(target)

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# TABLA FINAL DE MÉTRICAS
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
df_metrics = pd.DataFrame(metrics_summary)
print("\n Tabla resumen de métricas (Test):")
print(df_metrics.to_string(index=False))

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# MATRIZ DE CORRELACIÓN (OPCIONAL)
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
plt.figure(figsize=(10, 8))
corr = data.drop(columns=['Date']).corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f", square=True)
plt.title("Matriz de Correlación entre Variables")
plt.tight_layout()
plt.show()

# CURVA DE APRENDIZAJE

from sklearn.model_selection import learning_curve

def plot_learning_curve(estimator, X, y, title, cv, scoring='r2', n_jobs=2):
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=cv, scoring=scoring,
        train_sizes=np.linspace(0.1, 1.0, 5), n_jobs=n_jobs, shuffle=False
    )
    train_scores_mean = np.mean(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)

    plt.figure(figsize=(8,6))
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Cross-validation score")
    plt.title(title)
    plt.xlabel("Training examples")
    plt.ylabel(scoring)
    plt.legend(loc="best")
    plt.grid()
    plt.show()

print("\n Curvas de aprendizaje para cada variable objetivo:")

for target in targets:
    y_train = train_data[target]
    print(f"\n {target}")
    # Usa el mejor modelo con parámetros default o tuning
    rf_default = RandomForestRegressor(random_state=42)
    plot_learning_curve(
        rf_default,
        X_train,
        y_train,
        title=f"Curva de aprendizaje Random Forest - {target}",
        cv=TimeSeriesSplit(n_splits=3),
        scoring='r2',
        n_jobs=2
    )

from sklearn.model_selection import learning_curve

plt.figure(figsize=(10, 6))

colors = ['r', 'b', 'g']  # colores para cada variable
cv = TimeSeriesSplit(n_splits=3)

for i, target in enumerate(targets):
    y_train = train_data[target]
    rf_default = RandomForestRegressor(random_state=42)

    train_sizes, train_scores, test_scores = learning_curve(
        rf_default,
        X_train,
        y_train,
        cv=cv,
        scoring='r2',
        train_sizes=np.linspace(0.1, 1.0, 5),
        n_jobs=2,
        shuffle=False
    )

    train_scores_mean = np.mean(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)

    # Curva entrenamiento (línea continua)
    plt.plot(train_sizes, train_scores_mean, color=colors[i], linestyle='-', marker='o', label=f'{target} - Train')
    # Curva validación (línea punteada)
    plt.plot(train_sizes, test_scores_mean, color=colors[i], linestyle='--', marker='o', label=f'{target} - Test')

plt.title("Curvas de aprendizaje Random Forest - Variables objetivo")
plt.xlabel("Tamaño del conjunto de entrenamiento")
plt.ylabel("R²")
plt.legend()
plt.grid()
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# CARGA DE DATOS
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
file_path = "/content/DatosTempHKObs__Finales.xlsx"
data = pd.read_excel(file_path)
data['Date'] = pd.to_datetime(data['Date'])

# Eliminar columnas no deseadas
cols_to_drop = ['Punto de rocio', 'Year', 'Month', 'Day']
data = data.drop(columns=[col for col in cols_to_drop if col in data.columns])

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# DIVISIÓN ENTRE TRAIN Y TEST
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
train_data = data[data['Date'] < '2012-01-01']
test_data = data[data['Date'] >= '2012-01-01']

# Variables predictoras
features = ['Nubes', 'Humedad ', 'Precipitacion', 'Presion ']
X_train = train_data[features]
X_test = test_data[features]

# Variables objetivo
targets = ['MaxTemp', 'MinTemp', 'MeanTemp']

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# OPTIMIZACIÓN DE HYPERPARÁMETROS
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
tscv = TimeSeriesSplit(n_splits=3)
def tune_random_forest(X, y):
    param_grid = {
        'n_estimators': [100, 200],
        'max_depth': [15, 20],
        'min_samples_split': [2, 5],
        'min_samples_leaf': [1, 2],
        'max_features': ['sqrt', 'log2'],
        'bootstrap': [True, False]
    }

    rf = RandomForestRegressor(random_state=42)
    grid_search = GridSearchCV(rf, param_grid, cv=tscv, scoring='r2', n_jobs=2, verbose=1)
    grid_search.fit(X, y)

    print("Mejores parámetros encontrados:", grid_search.best_params_)
    return grid_search.best_estimator_

# Listas para métricas e importancias
metrics_summary = []
importance_dict = {}

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# FUNCIÓN DE RANDOM FOREST + MÉTRICAS Y GRÁFICOS
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
def evaluate_rf_model(y_col):
    y_train = train_data[y_col]
    y_test = test_data[y_col]

    print(f"\n Optimizando hiperparámetros para: {y_col}")
    rf = tune_random_forest(X_train, y_train)

    y_train_pred = rf.predict(X_train)
    y_test_pred = rf.predict(X_test)

    # MÉTRICAS
    def report_metrics(y_true, y_pred, label):
        r2 = r2_score(y_true, y_pred)
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        mask = y_true != 0
        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
        print(f"\n{label} - {y_col}")
        print(f"  R²:   {r2:.3f}")
        print(f"  MAE:  {mae:.3f}")
        print(f"  RMSE: {rmse:.3f}")
        print(f"  MAPE:  {mape:.2f}%")
        return r2, mae, rmse, mape

    r2_test, mae_test, rmse_test, mape_test = report_metrics(y_test, y_test_pred, "Test")
    report_metrics(y_train, y_train_pred, "Train")

    # Guardar métricas
    metrics_summary.append({
        "Variable": y_col,
        "R2_Test": round(r2_test, 3),
        "MAE_Test": round(mae_test, 3),
        "RMSE_Test": round(rmse_test, 3)
    })

    # Guardar importancia de variables
    importance_dict[y_col] = rf.feature_importances_

    # GRÁFICO: REAL VS PREDICHO (TEST)
    plt.figure(figsize=(10, 4))
    plt.plot(test_data['Date'], y_test.values, label="Real", color="steelblue")
    plt.plot(test_data['Date'], y_test_pred, label="Predicción", color="darkred")
    plt.fill_between(test_data['Date'], y_test_pred, y_test.values, color="gray", alpha=0.2, label="Error")
    plt.title(f"Predicción vs Real - {y_col}")
    plt.xlabel("Fecha")
    plt.ylabel("Valor")
    plt.legend()
    plt.tight_layout()
    plt.show()

    # GRÁFICO: ERRORES/RESIDUOS
    residuals = y_test.values - y_test_pred
    plt.figure(figsize=(8, 4))
    plt.plot(test_data['Date'], residuals, color='orange', label='Error')
    plt.axhline(0, color='black', linestyle='--')
    plt.title(f"Errores (Residuos) - {y_col}")
    plt.xlabel("Fecha")
    plt.ylabel("Error")
    plt.legend()
    plt.tight_layout()
    plt.show()

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# EVALUAR CADA VARIABLE OBJETIVO
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
for target in targets:
    evaluate_rf_model(target)

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# TABLA FINAL DE MÉTRICAS
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
df_metrics = pd.DataFrame(metrics_summary)
print("\n Tabla resumen de métricas (Test):")
print(df_metrics.to_string(index=False))

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# GRÁFICA DE IMPORTANCIA DE VARIABLES COMBINADA
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
importance_df = pd.DataFrame(importance_dict, index=features)
importance_df = importance_df.reset_index().rename(columns={'index': 'Variable'})

importance_df_melted = importance_df.melt(id_vars='Variable', var_name='Objetivo', value_name='Importancia')

plt.figure(figsize=(10, 6))
sns.barplot(data=importance_df_melted, x='Variable', y='Importancia', hue='Objetivo')
plt.title('Importancia de Variables por Variable Objetivo (Random Forest)')
plt.ylabel('Importancia')
plt.xlabel('Variable Predictora')
plt.legend(title='Variable Objetivo')
plt.tight_layout()
plt.show()

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# MATRIZ DE CORRELACIÓN (OPCIONAL)
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
plt.figure(figsize=(10, 8))
corr = data.drop(columns=['Date']).corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f", square=True)
plt.title("Matriz de Correlación entre Variables")
plt.tight_layout()
plt.show()

# Paleta de colores personalizada
custom_palette = {
    'MaxTemp': 'red',
    'MinTemp': 'blue',
    'MeanTemp': 'green'
}

plt.figure(figsize=(10, 6))
sns.barplot(data=importance_df_melted, x='Variable', y='Importancia', hue='Objetivo', palette=custom_palette)
plt.title('Importancia de Variables por Variable Objetivo (Random Forest)')
plt.ylabel('Importancia')
plt.xlabel('Variable Predictora')
plt.legend(title='Variable Objetivo')
plt.tight_layout()
plt.show()

"""**XGB con hiperparametros**

Comparativa MAE y MSE
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# CARGA DE DATOS
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
file_path = "/content/DatosTempHKObs__Finales.xlsx"
data = pd.read_excel(file_path)
data['Date'] = pd.to_datetime(data['Date'])

# Eliminar columnas no deseadas
cols_to_drop = ['Punto de rocio', 'Year', 'Month', 'Day']
data = data.drop(columns=[col for col in cols_to_drop if col in data.columns])

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# DIVISIÓN ENTRE TRAIN Y TEST
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
train_data = data[data['Date'] < '2012-01-01']
test_data = data[data['Date'] >= '2012-01-01']

features = data.columns.difference(['Date', 'MaxTemp', 'MinTemp', 'MeanTemp'])
X_train = train_data[features]
X_test = test_data[features]

targets = ['MaxTemp', 'MinTemp', 'MeanTemp']
tscv = TimeSeriesSplit(n_splits=3)
metrics_summary = []

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# FUNCIÓN DE EVALUACIÓN XGBOOST
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
def evaluate_xgboost_model(y_col, objective_name, scoring_metric):
    y_train = train_data[y_col]
    y_test = test_data[y_col]

    print(f"\n XGBoost ({objective_name}) para: {y_col}")

    model = xgb.XGBRegressor(objective=objective_name, n_jobs=-1, random_state=42)

    param_grid = {
        'max_depth': [3, 5],
        'learning_rate': [0.05, 0.1],
        'n_estimators': [100, 200],
        'subsample': [0.8, 1],
        'colsample_bytree': [0.8, 1]
    }

    grid_search = GridSearchCV(model, param_grid, cv=tscv, scoring=scoring_metric, verbose=0)
    grid_search.fit(X_train, y_train)

    best_model = grid_search.best_estimator_
    print(" Mejores parámetros:", grid_search.best_params_)

    y_train_pred = best_model.predict(X_train)
    y_test_pred = best_model.predict(X_test)

    # MÉTRICAS
    def report_metrics(y_true, y_pred, label):
        r2 = r2_score(y_true, y_pred)
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
         # MAPE (evita división por cero)
        mask = y_true != 0
        if np.any(mask):
            mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
        else:
            mape = np.nan  # si todos los valores reales son 0
        print(f"\n {label} - {y_col} ({objective_name})")
        print(f"  R²:   {r2:.3f}")
        print(f"  MAE:  {mae:.3f}")
        print(f"  RMSE: {rmse:.3f}")
        print(f"  MAPE: {mape:.2f}%")
        return r2, mae, rmse, mape

    r2_test, mae_test, rmse_test, mape_test = report_metrics(y_test, y_test_pred, "Test")
    report_metrics(y_train, y_train_pred, "Train")

    # Guardar métricas
    metrics_summary.append({
        "Variable": y_col,
        "Pérdida": objective_name,
        "R2_Test": round(r2_test, 3),
        "MAE_Test": round(mae_test, 3),
        "RMSE_Test": round(rmse_test, 3)
    })

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# EVALUAR TODAS LAS VARIABLES (MAE Y MSE)
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
for target in targets:
    evaluate_xgboost_model(target, objective_name='reg:absoluteerror', scoring_metric='neg_mean_absolute_error')
    evaluate_xgboost_model(target, objective_name='reg:squarederror', scoring_metric='neg_mean_squared_error')

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# TABLA FINAL DE MÉTRICAS
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
df_metrics = pd.DataFrame(metrics_summary)
print("\n Tabla resumen de métricas (Test):")
print(df_metrics.to_string(index=False))

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# CORRELACIÓN DE VARIABLES
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
plt.figure(figsize=(10, 8))
corr = data.drop(columns=['Date']).corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f", square=True)
plt.title("Matriz de Correlación entre Variables")
plt.tight_layout()
plt.show()

"""Importancia de las variables"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# CARGA DE DATOS
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
file_path = "/content/DatosTempHKObs__Finales.xlsx"
data = pd.read_excel(file_path)
data['Date'] = pd.to_datetime(data['Date'])

# Eliminar columnas no deseadas
cols_to_drop = ['Punto de rocio', 'Year', 'Month', 'Day']
data = data.drop(columns=[col for col in cols_to_drop if col in data.columns])

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# DIVISIÓN ENTRE TRAIN Y TEST
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
train_data = data[data['Date'] < '2012-01-01']
test_data = data[data['Date'] >= '2012-01-01']

features = data.columns.difference(['Date', 'MaxTemp', 'MinTemp', 'MeanTemp'])
X_train = train_data[features]
X_test = test_data[features]

targets = ['MaxTemp', 'MinTemp', 'MeanTemp']

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# OPTIMIZACIÓN Y EVALUACIÓN XGBOOST
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
tscv = TimeSeriesSplit(n_splits=3)
metrics_summary = []

def evaluate_xgboost_model(y_col):
    y_train = train_data[y_col]
    y_test = test_data[y_col]

    print(f"\n Optimizando XGBoost para: {y_col}")

    model = xgb.XGBRegressor(objective='reg:absoluteerror', n_jobs=-1, random_state=42)

    param_grid = {
      'max_depth': [3, 5, 7, 9],
      'learning_rate': [0.01, 0.05, 0.1, 0.2],
      'n_estimators': [100, 200, 300],
      'subsample': [ 0.8, 1],
      'colsample_bytree': [ 0.8, 1]
    }

    grid_search = GridSearchCV(model, param_grid, cv=tscv, scoring='r2', verbose=1)
    grid_search.fit(X_train, y_train)

    best_model = grid_search.best_estimator_
    print(" Mejores parámetros:", grid_search.best_params_)

    y_train_pred = best_model.predict(X_train)
    y_test_pred = best_model.predict(X_test)

    # MÉTRICAS
    def report_metrics(y_true, y_pred, label):
        r2 = r2_score(y_true, y_pred)
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        mask = y_true != 0
        if np.any(mask):
            mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
        else:
            mape = np.nan

        print(f"\n {label} - {y_col}")
        print(f"  R²:   {r2:.3f}")
        print(f"  MAE:  {mae:.3f}")
        print(f"  RMSE: {rmse:.3f}")
        print(f"  MAPE: {mape:.2f}%")
        return r2, mae, rmse, mape


    r2_test, mae_test, rmse_test, mape_test = report_metrics(y_test, y_test_pred, "Test")
    report_metrics(y_train, y_train_pred, "Train")

    # Guardar métricas
    metrics_summary.append({
        "Variable": y_col,
        "R2_Test": round(r2_test, 3),
        "MAE_Test": round(mae_test, 3),
        "RMSE_Test": round(rmse_test, 3)
    })

    # IMPORTANCIA DE VARIABLES
    importances = pd.Series(best_model.feature_importances_, index=X_train.columns).sort_values()
    plt.figure(figsize=(8, 5))
    importances.plot(kind='barh', color='forestgreen')
    plt.title(f"Importancia de Variables - {y_col}")
    plt.xlabel("Importancia")
    plt.tight_layout()
    plt.show()

    # REAL VS PREDICHO
    plt.figure(figsize=(10, 4))
    plt.plot(test_data['Date'], y_test.values, label="Real", color="steelblue")
    plt.plot(test_data['Date'], y_test_pred, label="Predicción", color="darkred")
    plt.fill_between(test_data['Date'], y_test_pred, y_test.values, color="gray", alpha=0.2, label="Error")
    plt.title(f"Predicción vs Real - {y_col}")
    plt.xlabel("Fecha")
    plt.ylabel("Valor")
    plt.legend()
    plt.tight_layout()
    plt.show()

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# EVALUAR TODAS LAS VARIABLES
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
for target in targets:
    evaluate_xgboost_model(target)

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# TABLA FINAL DE MÉTRICAS
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
df_metrics = pd.DataFrame(metrics_summary)
print("\n Tabla resumen de métricas (Test):")
print(df_metrics.to_string(index=False))

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# CORRELACIÓN DE VARIABLES
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
plt.figure(figsize=(10, 8))
corr = data.drop(columns=['Date']).corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f", square=True)
plt.title("Matriz de Correlación entre Variables")
plt.tight_layout()
plt.show()

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# IMPORTS Y CONFIGURACIÓN
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# CARGA DE DATOS
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
file_path = "/content/DatosTempHKObs__Finales.xlsx"
data = pd.read_excel(file_path)
data['Date'] = pd.to_datetime(data['Date'])

# Eliminar columnas no deseadas
cols_to_drop = ['Punto de rocio', 'Year', 'Month', 'Day']
data = data.drop(columns=[col for col in cols_to_drop if col in data.columns])

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# DIVISIÓN ENTRE TRAIN Y TEST
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
train_data = data[data['Date'] < '2012-01-01']
test_data = data[data['Date'] >= '2012-01-01']

features = data.columns.difference(['Date', 'MaxTemp', 'MinTemp', 'MeanTemp'])
X_train = train_data[features]
X_test = test_data[features]

targets = ['MaxTemp', 'MinTemp', 'MeanTemp']

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# OPTIMIZACIÓN Y EVALUACIÓN XGBOOST
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
tscv = TimeSeriesSplit(n_splits=3)
metrics_summary = []
importancia_total = []  # ⬅ Aquí se guardan las importancias de cada modelo

def evaluate_xgboost_model(y_col):
    y_train = train_data[y_col]
    y_test = test_data[y_col]

    print(f"\n Optimizando XGBoost para: {y_col}")

    model = xgb.XGBRegressor(objective='reg:absoluteerror', n_jobs=-1, random_state=42)

    param_grid = {
      'max_depth': [3, 5, 7, 9],
      'learning_rate': [0.01, 0.05, 0.1, 0.2],
      'n_estimators': [100, 200, 300],
      'subsample': [ 0.8, 1],
      'colsample_bytree': [ 0.8, 1]
    }

    grid_search = GridSearchCV(model, param_grid, cv=tscv, scoring='r2', verbose=1)
    grid_search.fit(X_train, y_train)

    best_model = grid_search.best_estimator_
    print(" Mejores parámetros:", grid_search.best_params_)

    y_train_pred = best_model.predict(X_train)
    y_test_pred = best_model.predict(X_test)

    # MÉTRICAS
    def report_metrics(y_true, y_pred, label):
        r2 = r2_score(y_true, y_pred)
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        mask = y_true != 0
        if np.any(mask):
            mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
        else:
            mape = np.nan

        print(f"\n {label} - {y_col}")
        print(f"  R²:   {r2:.3f}")
        print(f"  MAE:  {mae:.3f}")
        print(f"  RMSE: {rmse:.3f}")
        print(f"  MAPE: {mape:.2f}%")
        return r2, mae, rmse, mape

    r2_test, mae_test, rmse_test, mape_test = report_metrics(y_test, y_test_pred, "Test")
    report_metrics(y_train, y_train_pred, "Train")

    # Guardar métricas
    metrics_summary.append({
        "Variable": y_col,
        "R2_Test": round(r2_test, 3),
        "MAE_Test": round(mae_test, 3),
        "RMSE_Test": round(rmse_test, 3)
    })

    # IMPORTANCIA DE VARIABLES (individual)
    importances = pd.Series(best_model.feature_importances_, index=X_train.columns).sort_values()
    plt.figure(figsize=(8, 5))
    importances.plot(kind='barh', color='forestgreen')
    plt.title(f"Importancia de Variables - {y_col}")
    plt.xlabel("Importancia")
    plt.tight_layout()
    plt.show()

    # ⬅ Añadir al dataframe total para gráfica conjunta
    importancia_df = importances.rename("Importancia").to_frame().reset_index().rename(columns={"index": "Variable"})
    importancia_df["Objetivo"] = y_col
    importancia_total.append(importancia_df)

    # REAL VS PREDICHO
    plt.figure(figsize=(10, 4))
    plt.plot(test_data['Date'], y_test.values, label="Real", color="steelblue")
    plt.plot(test_data['Date'], y_test_pred, label="Predicción", color="darkred")
    plt.fill_between(test_data['Date'], y_test_pred, y_test.values, color="gray", alpha=0.2, label="Error")
    plt.title(f"Predicción vs Real - {y_col}")
    plt.xlabel("Fecha")
    plt.ylabel("Valor")
    plt.legend()
    plt.tight_layout()
    plt.show()

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# EVALUAR TODAS LAS VARIABLES
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
for target in targets:
    evaluate_xgboost_model(target)

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# TABLA FINAL DE MÉTRICAS
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
df_metrics = pd.DataFrame(metrics_summary)
print("\n Tabla resumen de métricas (Test):")
print(df_metrics.to_string(index=False))

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# GRAFICO CONJUNTO DE IMPORTANCIAS
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
df_importancias_final = pd.concat(importancia_total, ignore_index=True)

plt.figure(figsize=(12, 6))
sns.barplot(data=df_importancias_final, x="Variable", y="Importancia", hue="Objetivo")
plt.title("Importancia de Variables por Variable Objetivo (XGBoost)")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# CORRELACIÓN DE VARIABLES
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
plt.figure(figsize=(10, 8))
corr = data.drop(columns=['Date']).corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f", square=True)
plt.title("Matriz de Correlación entre Variables")
plt.tight_layout()
plt.show()

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# GRAFICO CONJUNTO DE IMPORTANCIAS CON COLORES PERSONALIZADOS
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
df_importancias_final = pd.concat(importancia_total, ignore_index=True)

# Paleta de colores por variable objetivo
palette_custom = {
    "MaxTemp": "red",   # rojo
    "MinTemp": "blue",   # azul
    "MeanTemp": "green"   # verde
}

plt.figure(figsize=(12, 6))
sns.barplot(
    data=df_importancias_final,
    x="Variable",
    y="Importancia",
    hue="Objetivo",
    palette=palette_custom
)
plt.title("Importancia de Variables por Variable Objetivo (XGBoost)")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""**Comparativa de los modelos**"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Cargar datos desde Excel
ruta_excel = '/content/TablaDeMetricas.xlsx'  # ← Asegúrate de que esta ruta sea válida
hoja = 'Hoja1'
df = pd.read_excel(ruta_excel, sheet_name=hoja, index_col=0)

# Extraer métricas y modelos
metricas = df.index.tolist()     # ['R2', 'MAE', 'RMSE']
modelos = df.columns.tolist()    # ['Modelo A', 'Modelo B', 'Modelo C']
valores = [df[modelo].values for modelo in modelos]  # lista de listas: una por modelo

# Posiciones y ancho
n_metricas = len(metricas)
n_modelos = len(modelos)
ancho = 0.35  # Barras más delgadas para mejor separación

# Para separar más entre grupos, ampliamos la distancia entre métricas:
# En lugar de usar np.arange(n_metricas), usamos np.arange(n_metricas)*factor_espaciado
factor_espaciado = 2  # Aumenta para mayor separación entre grupos

x = np.arange(n_metricas) * factor_espaciado  # posiciones base para grupos de métricas

# Graficar
fig, ax = plt.subplots(figsize=(12, 6))

for i, (modelo, valores_modelo) in enumerate(zip(modelos, valores)):
    ax.bar(x + i * ancho, valores_modelo, width=ancho, label=modelo)

# Personalización
ax.set_xlabel('Métrica')
ax.set_ylabel('Valor')
ax.set_title('Comparación de Métricas por Modelo')
# Ajustamos xticks para que queden centrados en cada grupo
ax.set_xticks(x + (ancho * (n_modelos - 1)) / 2)
ax.set_xticklabels(metricas)
ax.legend(title='Modelo')

plt.tight_layout()
plt.show()